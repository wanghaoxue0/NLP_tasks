{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A guidance for automatic annotation for biomodels\n",
    "### by Haoxue Wang \n",
    "\n",
    "### There are three stages of our automatic annotation procedure\n",
    "* prepare the pmc link, download the full text in .txt if it is open source, otherwise obtain the abstract only\n",
    "* use our algorithms to do the automatic annotation with matching ontology, output saved as json file\n",
    "* transfer the output into readable .csv file, which can be compared with ground-true labels if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 prepare the text\n",
    "\n",
    "For biomodels, we can get the models' corresponding pmc id through official API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# get all the curated biomodels api url\n",
    "base_url = \"https://www.ebi.ac.uk/biomodels/BIOMD\"\n",
    "start_index = 1\n",
    "end_index = 1073\n",
    "\n",
    "\n",
    "# Create the list of API URLs\n",
    "urls = [f\"{base_url}{str(i).zfill(10)}?format=json\" for i in range(start_index, end_index + 1)]\n",
    "# store training data\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[\"publicationId\", \"description\", \"abstract\", \"link\"])\n",
    "for url in urls:\n",
    "  # Perform the API request\n",
    "  try:\n",
    "      print(\"fetch the data for\", url)\n",
    "      response = requests.get(url)\n",
    "      data = response.json()\n",
    "      publicationId = data['publicationId']\n",
    "      # Parse the HTML data using BeautifulSoup\n",
    "      soup = BeautifulSoup(data['description'], 'html.parser')\n",
    "      description = soup.get_text()\n",
    "      abstract = data['publication']['synopsis']\n",
    "      link=data['publication']['link']\n",
    "      df = df.append({\"publicationId\":publicationId, \"description\": description, \"abstract\" :abstract, \"link\": link}, ignore_index=True)\n",
    "  except:\n",
    "      pass\n",
    "  \n",
    "def pubmed_to_pmc(pubmed_link):\n",
    "    return pubmed_link.replace(\"http://identifiers.org/pubmed/\", \"pmc\")\n",
    "\n",
    "# there are other links from non pubmed, check them manually if needed\n",
    "pubmed_rows = df[df['link'].str.contains('pubmed', case=False)]\n",
    "# List of PubMed links\n",
    "pubmed_links= pubmed_rows[\"link\"]\n",
    "\n",
    "# Loop through the PubMed links and convert to PMC format\n",
    "pmc_links = [pubmed_to_pmc(link) for link in pubmed_links]\n",
    "result_df = pd.concat([pubmed_rows, pd.DataFrame(pmc_links)], axis=1)\n",
    "pmc_ids = list(set(pmc_links))  # the unique id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the above coding, we can get the abstract and the description from the Biomodels directly\n",
    "data = pd.DataFrame(df)\n",
    "abstract = data['abstract']\n",
    "description = data['description']\n",
    "data.to_csv('data_ebi.csv')\n",
    "\n",
    "\n",
    "# Using the result from csv\n",
    "data = pd.read_csv('data_ebi.csv')\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def create_txt_files(csv_file, save_folder):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "\n",
    "        # Skip the header row\n",
    "        header = next(csvreader)\n",
    "\n",
    "        for row in csvreader:\n",
    "            abstract = row[2]  # Assuming 'abstract' column is at index 2 (0-based index)\n",
    "            name = row[0]  # Assuming '0' column is at index 4 (0-based index)\n",
    "\n",
    "            txt_filename = os.path.join(save_folder, f'{name}.txt')\n",
    "            with open(txt_filename, 'w') as txtfile:\n",
    "                txtfile.write(abstract)\n",
    "\n",
    "# Usage example:\n",
    "csv_file = 'data_ebi.csv'\n",
    "save_folder = 'data'\n",
    "create_txt_files(csv_file, save_folder)\n",
    "# Thus we download each abstract as the content of .txt, with the biomodels ID as the file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have the pmc id, we need to download the full text/abstract through official European PMC API/ PubMed Central® (PMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the full text through PubMed Central® (PMC)\n",
    "def get_full_text(pmc_id):\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/{pmc_id[3:]}/unicode\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch the full-text for PMC ID: {pmc_id}\")\n",
    "        return None\n",
    "    \n",
    "def extract_all_text(json_data):\n",
    "    text_content = ''\n",
    "    if isinstance(json_data, dict):\n",
    "        for key, value in json_data.items():\n",
    "            text_content += extract_all_text(value)\n",
    "    elif isinstance(json_data, list):\n",
    "        for item in json_data:\n",
    "            text_content += extract_all_text(item)\n",
    "    elif isinstance(json_data, str):\n",
    "        text_content += json_data + ' '\n",
    "\n",
    "    return text_content    \n",
    "\n",
    "save_path = '/Users/haoxuewang/haoxue_pytorch/data/texts'\n",
    "\n",
    "for pmc_id in pmc_ids:\n",
    "    full_text = get_full_text(pmc_id)\n",
    "    if full_text:\n",
    "        text_content = extract_all_text(full_text)\n",
    "        file_name = f'{os.path.basename(pmc_id)}.txt'\n",
    "        with open(os.path.join(save_path, file_name), 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(text_content)\n",
    "        \n",
    "        print(f\"Converted to .txt file: {os.path.join(save_path, file_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the full text through official European PMC API, it is slightly different as we need to transfer the xml into .txt\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual API key\n",
    "API_KEY = '9d4ee27f78c8c5d685ab9251061512dfc708'\n",
    "BASE_URL = 'https://www.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi'\n",
    "\n",
    "def get_full_text(pmc_id):\n",
    "    # url = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/{pmc_id[3:]}/unicode\"\n",
    "    url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/{pmc_id}/fullTextXML\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch the full-text for PMC ID: {pmc_id}\")\n",
    "        return None\n",
    "    \n",
    "def extract_all_text_from_xml(xml_data):\n",
    "    if xml_data is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(xml_data)\n",
    "        all_text = ' '.join(element.text.strip() for element in root.iter() if element.text)\n",
    "        return all_text\n",
    "    except ET.ParseError:\n",
    "        print(\"Error parsing XML data.\")\n",
    "        return None   \n",
    "\n",
    "\n",
    "\n",
    "save_path = '/Users/your_own_path'\n",
    "for pmc_id in pmc_ids:\n",
    "    full_text = get_full_text(pmc_id)\n",
    "    if full_text:\n",
    "        text_content = extract_all_text_from_xml(full_text)\n",
    "        file_name = f'{os.path.basename(pmc_id)}.txt'\n",
    "        with open(os.path.join(save_path, file_name), 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(text_content)\n",
    "        \n",
    "        print(f\"Converted to .txt file: {os.path.join(save_path, file_name)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 automatic annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic annation for biomodels\n",
    "# by Haoxue Wang\n",
    "\n",
    "import spacy\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_spacy(file_path):\n",
    "    nlp = spacy.load(\"en_core_sci_sm\")\n",
    "    # nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "    with open(file_path, \"r\") as file:          \n",
    "        texts = file.readlines()\n",
    "    processed_data = []\n",
    "    unique_entities = set()\n",
    "    for i, text in enumerate(texts):\n",
    "        doc = nlp(text)\n",
    "        # Extract entities from the document\n",
    "        entities = [{\"entity\": ent.text} for ent in doc.ents]\n",
    "        unique_entities.update(ent.text for ent in doc.ents)\n",
    "        # Add the processed data for this text to the list\n",
    "        processed_data.append({\"entities\": entities})\n",
    "    return processed_data\n",
    "\n",
    "def search_ols(query):\n",
    "    base_url = \"https://www.ebi.ac.uk/ols/api/select\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"groupField\": \"iri\",\n",
    "        \"start\": 0\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Error accessing OLS API. Status code: {response.status_code}\")\n",
    "\n",
    "def get_first_result_info(query):\n",
    "    try:\n",
    "        data = search_ols(query)\n",
    "        if data[\"response\"][\"numFound\"] == 0:\n",
    "            return\n",
    "        first_result = data[\"response\"][\"docs\"][0]\n",
    "        label = first_result[\"label\"]\n",
    "        # bad_ontology = ['bao', 'cco','afo',\"snomed\",\"dicom\",\"reproduceme\",\"vo\"]\n",
    "        good_ontology = [\"taxonomy\",\"go\",\"more\",\"ncit\",\"bto\",\"reactome\",\"hdo\",\"chebi\",\"fma\",\"efo\",\"po\",\"hpo\",\"cto\",\"kc\",\"kd\",\"ofbi\",\"eo\",\"vario\",\"uk\",\"sbo\",\"ko\",\"kp\",\"en\",\"teddy\",\"eco\"]\n",
    "        bad_label = [',']\n",
    "        if has_three_or_less_words(label):\n",
    "            if label not in bad_label:\n",
    "                ontology = first_result[\"ontology_name\"]\n",
    "                if ontology in good_ontology:\n",
    "                    iri = first_result[\"iri\"]\n",
    "        \n",
    "                    result_info = {\n",
    "                        \"label\": label,\n",
    "                        \"ontology\": ontology,\n",
    "                        \"iri\": iri\n",
    "                        }\n",
    "                    return result_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "def has_three_or_less_words(input_string):\n",
    "    # Split the string into words\n",
    "    words = input_string.split()\n",
    "\n",
    "    # Count the number of words\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Compare the count with the threshold\n",
    "    if num_words < 4:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def isnot_common_word(word):\n",
    "    common_words = {\"functional\",\"trends\"} # replace the common words in your own list\n",
    "\n",
    "    return word.lower() not in common_words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt_dir='/Users/haoxuewang/Desktop/haoxue_ebi/abstract/'\n",
    "txt_dir='/Users/haoxuewang/Desktop/haoxue_ebi/full_text/'\n",
    "input_directory=Path(txt_dir,'texts')\n",
    "output_directory=Path(txt_dir,'annotation')\n",
    "file_names = os.listdir(input_directory)\n",
    "models_name = [os.path.splitext(os.path.basename(file_name))[0] for file_name in file_names if file_name.endswith('.txt')]\n",
    "\n",
    "for model_name in models_name:\n",
    "    input_path = os.path.join(input_directory, f\"{model_name}.txt\")\n",
    "    words= get_spacy(input_path)\n",
    "    result=[]\n",
    "    for word in words:\n",
    "        entities = word[\"entities\"]\n",
    "        unique_entities = set(item['entity'] for item in entities)\n",
    "        unique_entities=list(unique_entities)\n",
    "        for entity in unique_entities:\n",
    "            search_query=entity\n",
    "            if isnot_common_word(search_query):\n",
    "                result.append(get_first_result_info(search_query))      \n",
    "    # Save the 'pure_list' as a JSON file with the same name as the input file\n",
    "    output_file_path = Path(output_directory, f\"{model_name}.json\")\n",
    "    print(output_file_path)\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        json.dump(result, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will get the file with json file in the following strcture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"label\": \"INTACT\",\n",
    "        \"ontology\": \"efo\",\n",
    "        \"iri\": \"http://www.ebi.ac.uk/efo/EFO_0010037\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"tetrahydrofolate\",\n",
    "        \"ontology\": \"chebi\",\n",
    "        \"iri\": \"http://purl.obolibrary.org/obo/CHEBI_67016\"\n",
    "    },\n",
    "    null,\n",
    "    {\n",
    "        \"label\": \"Pharmacokinetic Parameters Domain\",\n",
    "        \"ontology\": \"ncit\",\n",
    "        \"iri\": \"http://purl.obolibrary.org/obo/NCIT_C49607\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Enzyme\",\n",
    "        \"ontology\": \"ncit\",\n",
    "        \"iri\": \"http://purl.obolibrary.org/obo/NCIT_C16554\"\n",
    "    },\n",
    "    null,\n",
    "    {\n",
    "        \"label\": \"tetrahydrofolate interconversion\",\n",
    "        \"ontology\": \"go\",\n",
    "        \"iri\": \"http://purl.obolibrary.org/obo/GO_0035999\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"purine\",\n",
    "        \"ontology\": \"chebi\",\n",
    "        \"iri\": \"http://purl.obolibrary.org/obo/CHEBI_35584\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 transfer the output to .csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have the test label to compare the coverage \n",
    "# test the data coverage\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('metadata.csv')\n",
    "df = data.groupby('BIOMD')['comment'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "aggregated_df = df.groupby('BIOMD')['comment'].apply(lambda x: x.str.split()).reset_index()\n",
    "\n",
    "# Print the aggregated DataFrame\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can calculate the coverage with the test labels if avilable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "def read_json_file(json_file_path):\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as jsonfile:\n",
    "                data = json.load(jsonfile)\n",
    "                labels = []\n",
    "                for i in range(len(data)):\n",
    "                    if data[i] is not None:\n",
    "                        label = data[i]['label'].split()\n",
    "                        labels.append(label)\n",
    "                label_list=list(chain(*labels))\n",
    "        return label_list        \n",
    "    except FileNotFoundError:\n",
    "    # Ignore the error and continue with the rest of the code\n",
    "        pass\n",
    "\n",
    "\n",
    "def find_overlapping_elements(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    overlapping_elements = set1.intersection(set2)\n",
    "    return list(overlapping_elements)\n",
    "\n",
    "\n",
    "txt_dir='/Users/haoxuewang/Desktop/haoxue_ebi/abstract/'\n",
    "input_directory=Path(txt_dir,'annotation')\n",
    "result=[]\n",
    "for i in range(len(aggregated_df)):\n",
    "    try:\n",
    "        model_name = aggregated_df['BIOMD'][i]\n",
    "        input_path = os.path.join(input_directory, f\"{model_name}.json\")\n",
    "        label_texts= read_json_file(input_path)\n",
    "        lowercase_label_texts = [item.lower() for item in label_texts]\n",
    "        true_list = [item for item in aggregated_df['comment'][i] if item != '-']\n",
    "        lowercase_true_list = [item.lower() for item in true_list]\n",
    "        overlapping= find_overlapping_elements(lowercase_label_texts,lowercase_true_list)\n",
    "        coverage = len(overlapping)/len(aggregated_df['comment'][i])\n",
    "        result.append((model_name, coverage))\n",
    "    except (ValueError, TypeError):\n",
    "        # Ignore the error and continue with the loop\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need the transfer the output in a csv format\n",
    "#### For the below code, you change the .csv structure easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the table \n",
    "def check_json_file(json_file_path):\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as jsonfile:\n",
    "                data = json.load(jsonfile)\n",
    "                labels = []\n",
    "                iris=[]\n",
    "                for i in range(len(data)):\n",
    "                    if data[i] is not None:\n",
    "                        label = data[i]['label']\n",
    "                        iri =data[i]['iri'] \n",
    "                        labels.append(label)\n",
    "                        iris.append(iri)\n",
    "                label_list=list(labels)\n",
    "                iri_list=list(iris)\n",
    "\n",
    "        return label_list, iri_list        \n",
    "    except FileNotFoundError:\n",
    "    # Ignore the error and continue with the rest of the code\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the data coverage\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('metadata.csv')\n",
    "df = data.groupby('BIOMD')['comment'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "aggregated_df = df.groupby('BIOMD')['comment'].apply(lambda x: x.str.split()).reset_index()\n",
    "new_df=data.groupby('BIOMD')['value'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "# Print the aggregated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests=[]\n",
    "for i in range(len(result)):\n",
    "    if result[i][1] >= 0:\n",
    "        tests.append(result[i][0])\n",
    "txt_dir='/Users/haoxuewang/Desktop/haoxue_ebi/abstract/'\n",
    "table =[]\n",
    "for test in tests:\n",
    "    input_directory=Path(txt_dir,'annotation')\n",
    "    input_path = os.path.join(input_directory, f\"{test}.json\")\n",
    "    label_texts,iri_texts = check_json_file(input_path)\n",
    "    which = df['BIOMD']== test\n",
    "    label_true=list(df['comment'][which])\n",
    "    iri_true=list(new_df['value'][which])\n",
    "    table.append((test,label_texts,iri_texts,label_true,iri_true))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.DataFrame(table)\n",
    "table_annotation = pd.merge(pd.DataFrame(result), table, on=0, how='inner')\n",
    "table_annotation\n",
    "new_columns = {0: 'Biomodel name', '1_x': 'coverage', '1_y':'automatic annotation', 2:'ontology for automatic annotation', 3:'manual annotation', 4:'ontology for manual annotation'}\n",
    "table_annotation.rename(columns=new_columns, inplace=True)\n",
    "table_annotation\n",
    "sorted_df = table_annotation.sort_values(by='coverage', ascending=False)\n",
    "sorted_df\n",
    "\n",
    "sorted_df.to_csv('sorted_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a supplement material\n",
    "\n",
    "This is to show the procedure to obtain useful ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('metadata.csv')\n",
    "df = data.groupby('BIOMD')['ontology'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "aggregated_df = df.groupby('BIOMD')['ontology'].apply(lambda x: x.str.split()).reset_index()\n",
    "\n",
    "def process_string(input_string):\n",
    "    words = input_string.split()  # Split the input string into words\n",
    "    result = []\n",
    "    res=[]\n",
    "    if len(words) == 1:\n",
    "        words = ''.join([char.lower() for sublist in words for char in sublist]) \n",
    "        result.append(words)\n",
    "    else:    \n",
    "        for word in words:\n",
    "            res.append(word[0])  \n",
    "        res = ''.join([char.lower() for sublist in res for char in sublist])    \n",
    "        result.append(res)    \n",
    "    return result\n",
    "\n",
    "abb=[]\n",
    "for i in range(len(data)):\n",
    "    ontology=data['ontology'][i]\n",
    "    ont = process_string(ontology)\n",
    "    abb.append(ont)\n",
    "\n",
    "abbreviation= pd.DataFrame(abb)\n",
    "data1 = pd.concat([data,abbreviation], axis=1)\n",
    "data1\n",
    "\n",
    "unique_df = abbreviation.drop_duplicates().to_string(index=False)\n",
    "print(unique_df)\n",
    "good_ono=[\"taxonomy\",\"go\",\"more\",\"ncit\",\"bto\",\"reactome\",\"hdo\",\"chebi\",\"fma\",\"efo\",\"po\",\"hpo\",\"cto\",\"kc\",\"kd\",\"ofbi\",\"eo\",\"vario\",\"uk\",\"sbo\",\"ko\",\"kp\",\"en\",\"teddy\",\"eco\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
